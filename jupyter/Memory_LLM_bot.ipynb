{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv('GROQ_API')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Kumar Dahal, it's nice to meet you!\\n\\nBeing a junior AI engineer is an exciting field to be in. What are you working on these days? Are there any particular areas of AI that you're most interested in? \\n\\nI'm always happy to chat about AI, learn about new projects, and maybe even help out if I can. üòä  \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 24, 'total_tokens': 106, 'completion_time': 0.166942619, 'prompt_time': 0.002514357, 'queue_time': None, 'total_time': 0.16945697599999998}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-6dfacdaa-5b4d-42d1-a5cc-7059b551dc2b-0', usage_metadata={'input_tokens': 24, 'output_tokens': 82, 'total_tokens': 106})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi, i am kumar dahal junior AI engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You said your name is Kumar and that you are a data scientist!  üòä  Is there anything else you'd like to talk about?\\n\", response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 55, 'total_tokens': 87, 'completion_time': 0.064, 'prompt_time': 0.003142316, 'queue_time': None, 'total_time': 0.06714231600000001}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-eebf91a2-199e-4b4c-a958-6417b6dbbfc5-0', usage_metadata={'input_tokens': 55, 'output_tokens': 32, 'total_tokens': 87})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "\n",
    "    [\n",
    "        HumanMessage(content='Hi, my name is kumar and i am data scientist'),\n",
    "        AIMessage(content=\"Hello kumar! It is nice to meet you\"),\n",
    "        HumanMessage(content=\"hey, what is my name and what i am?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remebering the session at different level of different LLM, we use message history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Message history can use a message history class to wrap our model and make it stateful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it uses langchain community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "#all the chat should be added in chat history, also there is bunch of user \n",
    "#having their own history we need to have unique id or session history ID\n",
    "store = {}\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "#runnable use hostory and model to distinguish\n",
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hard coding sesson_id to test\n",
    "config = {\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Kumar! It's nice to meet you. \\n\\nWhat can I do for you today? Are you working on any interesting projects? \\n\\nI'm always eager to learn more about data science applications.\\n\", response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 25, 'total_tokens': 72, 'completion_time': 0.094857059, 'prompt_time': 0.002630677, 'queue_time': None, 'total_time': 0.09748773599999999}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-7caf9eff-26c2-49fa-8e0b-cc564e7fdf29-0', usage_metadata={'input_tokens': 25, 'output_tokens': 47, 'total_tokens': 72})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with_message_history.invoke(\n",
    "     [\n",
    "        HumanMessage(content='Hi, my name is kumar and i am data scientist')\n",
    "     ],\n",
    "     config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Kumar.  \\n\\nI remember that you told me at the beginning of our conversation. üòä  Is there anything else I can help you with? \\n', response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 85, 'total_tokens': 121, 'completion_time': 0.072, 'prompt_time': 0.004902964, 'queue_time': None, 'total_time': 0.07690296399999999}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-04fea11b-178a-4a7e-be0c-a190b618525f-0', usage_metadata={'input_tokens': 85, 'output_tokens': 36, 'total_tokens': 121})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with_message_history.invoke(\n",
    "     [\n",
    "        HumanMessage(content='What is my name?')\n",
    "     ],\n",
    "     config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Kumar, a data scientist.  \\n\\nThat's what you introduced yourself as!  \\n\\nIs there anything specific you'd like to know about being a data scientist, or perhaps something you're working on right now? I'm happy to chat further. \\n\", response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 133, 'total_tokens': 193, 'completion_time': 0.121852055, 'prompt_time': 0.006013892, 'queue_time': None, 'total_time': 0.127865947}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-3623ad7a-f8a9-4ca3-af88-a2a80b1ca7ba-0', usage_metadata={'input_tokens': 133, 'output_tokens': 60, 'total_tokens': 193})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with_message_history.invoke(\n",
    "     [\n",
    "        HumanMessage(content='who am i?')\n",
    "     ],\n",
    "     config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi John! It's nice to meet you. \\n\\nWhat kind of data science work do you do? Are you working on any exciting projects right now?  \\n\\nI'm always eager to learn more about how people are using data science to make a difference. üòä \\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the config --> session ID\n",
    "config1 ={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "     [\n",
    "        HumanMessage(content='Hi, my name is John and i am data scientist')\n",
    "     ],\n",
    "     config=config1,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Kumar, a data scientist.  \\n\\nIs there something in particular making you question that? I'm here to help if you're curious about something specific related to your identity as a data scientist, or if you'd like to explore other aspects of who you are. \\n\\nRemember, I'm still learning and growing, so I might not always have the perfect answer.  But I'll do my best to assist you!\\n\", response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 205, 'total_tokens': 300, 'completion_time': 0.196223992, 'prompt_time': 0.00816311, 'queue_time': None, 'total_time': 0.20438710200000001}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e0a35d09-f69c-471f-8f51-b9274218785d-0', usage_metadata={'input_tokens': 205, 'output_tokens': 95, 'total_tokens': 300})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with_message_history.invoke(\n",
    "     [\n",
    "        HumanMessage(content='who am i?')\n",
    "     ],\n",
    "     config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are John, a data scientist!  \\n\\nIs there anything else you'd like to tell me about yourself, or would you like to talk about data science?  üòÑ \\n\", response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 97, 'total_tokens': 137, 'completion_time': 0.080306579, 'prompt_time': 0.004754024, 'queue_time': None, 'total_time': 0.085060603}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f950b1e7-09af-463c-bdf3-e0d942f04273-0', usage_metadata={'input_tokens': 97, 'output_tokens': 40, 'total_tokens': 137})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with_message_history.invoke(\n",
    "     [\n",
    "        HumanMessage(content='who am i?')\n",
    "     ],\n",
    "     config=config1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using prompt template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\",\"You are a helpful assistant.Answer all the question to the besto of your ability \"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain  =prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Don! It's nice to meet you. \\n\\nI'm just a language model, so I don't have a physical presence. I exist as code running on computers.  \\n\\nWhat can I help you with? üòä  \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 42, 'total_tokens': 96, 'completion_time': 0.109136164, 'prompt_time': 0.002831307, 'queue_time': None, 'total_time': 0.111967471}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-3d550d71-9d15-4b20-b9ec-4b5c26502fd3-0', usage_metadata={'input_tokens': 42, 'output_tokens': 54, 'total_tokens': 96})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content='Hi my name is Don, i am coming for you')]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Don, it's nice to meet you! üëã \\n\\nWhat can I help you with today?  \\n\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi my name is don\")],\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding complexity\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\",\n",
    "     \"You are a helpful assistant.Answer all the question to the best of your ability in {language} \"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain  =prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is Don\")],\"language\":\"Nepali\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§°‡•ã‡§®! üòä  ‡§¨‡§ß‡§æ‡§à ‡§π‡•ã, ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡§æ‡§Æ ‡§•‡§æ‡§π‡§æ ‡§™‡§æ‡§è‡§ï‡•ã ‡§õ‡•Å‡•§ ‡§ï‡•á ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§ï‡§∏‡•à ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞‡§Æ‡§æ ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§õ‡•Å?\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing the conversation History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trim_message , helper to reduce how many messages we're sending to the model.trimmer specify how many tokens  we want to keep along with other parameters like we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you 're a good assistant\"),\n",
       " HumanMessage(content=\"hi! I'm bob\"),\n",
       " AIMessage(content='hi!'),\n",
       " HumanMessage(content='I like vanilla ice cream'),\n",
       " AIMessage(content='nice'),\n",
       " HumanMessage(content='whats 2+2'),\n",
       " AIMessage(content='4'),\n",
       " HumanMessage(content='thanks'),\n",
       " AIMessage(content='no problem!'),\n",
       " HumanMessage(content='Having fun?'),\n",
       " AIMessage(content='yes')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trimmer = trim_messages(\n",
    "    max_tokens = 70,\n",
    "    strategy = \"last\",\n",
    "    token_counter = model,\n",
    "    include_system = True,\n",
    "    allow_partial = False,\n",
    "    start_on =  \"human\"\n",
    "    )\n",
    "messages = [\n",
    "    SystemMessage(content = \"you 're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2+2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"Having fun?\"),\n",
    "    AIMessage(content=\"yes\"),\n",
    "\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You said you like vanilla ice cream! üòä üç¶\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#passing it through chain\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain  =(\n",
    "    RunnablePassthrough.assign(messages = itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "response = chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content = \"what ice cream do i like\")],\n",
    "    \"language\":\"english\"\n",
    "    }\n",
    ")\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"what is 2+2\".  \\n\\nIs there anything else I can help you with?\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content = \"what math problem have i asked?\")],\n",
    "    \"language\":\"english\"\n",
    "    }\n",
    ")\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrapping this into message history\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to past conversations or any personal information about you, including your name. \\n\\nIf you'd like to tell me your name, I'd be happy to remember it! üòä  \\n\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response  = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\":[HumanMessage(content=\"waht is my name\")],\n",
    "        \"language\":\"English\",\n",
    "    },\n",
    "    config = config\n",
    "\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
